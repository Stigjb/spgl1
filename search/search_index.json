{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SPGL1: A solver for sparse least squares version 2.1 released December 2019 This latest release of SPGL1 implements a dual root-finding mode that allows for increased accuracy for basis pusuit denoising problems. See Theory . SPGL1 is an open-source Matlab solver for sparse least-squares. It is designed to solve any one of these three problem formulations: Lasso problem \\mathop{\\mathrm{minimize}}_{x}\\quad {\\textstyle\\frac{1}{2}}\\Vert Ax-b\\Vert_2^2\\quad\\mathrm{subject\\ to}\\quad \\Vert x\\Vert_p \\leq \\tau \\mathop{\\mathrm{minimize}}_{x}\\quad {\\textstyle\\frac{1}{2}}\\Vert Ax-b\\Vert_2^2\\quad\\mathrm{subject\\ to}\\quad \\Vert x\\Vert_p \\leq \\tau Basis pursuit denoise \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_p\\quad \\mathrm{subject\\ to}\\quad \\Vert Ax-b\\Vert_2 \\leq \\sigma \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_p\\quad \\mathrm{subject\\ to}\\quad \\Vert Ax-b\\Vert_2 \\leq \\sigma Basis pursuit \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_p\\quad \\mathrm{subject\\ to}\\quad Ax=b \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_p\\quad \\mathrm{subject\\ to}\\quad Ax=b Features Linear operator A. SPGL1 relies on matrix-vector operations A*x and A'*y , and accepts both explicit matrices (dense or sparse) and functions that evaluate these products. Sparsity regularizer. Any sparsity-inducing norm \\Vert\\cdot\\Vert_p \\Vert\\cdot\\Vert_p can be used if the user provides functions evaluating the primal norm, the corresponding dual norm, and a procedure for projecting onto a level-set of the primal norm. The default is the 1-norm. Several other norms included in SPGL1 are the group (1,2)-norm and the special multiple-measurement vector (MMV) case. Real and complex domains. SPGL1 is suitable for problems that live in either the real or complex domains. In the complex domain, the correct corresponding 1-norm (sum of magnitudes) is used. Feedback We are glad to hear from you if you find SPGL1 useful, or if you have any suggestions, contributions, or bug reports. Please send these to the SPGL1 authors Ewout van den Berg (Email <vandenberg.ewout@gmail.com> ) Michael P. Friedlander (Email: <mpf@cs.ubc.ca> ) Credits This research is supported in part by an NSERC Discovery Grant and by a grant for the Office of Naval Research (N00014-17-1-2009).","title":"Introduction"},{"location":"#spgl1-a-solver-for-sparse-least-squares","text":"version 2.1 released December 2019 This latest release of SPGL1 implements a dual root-finding mode that allows for increased accuracy for basis pusuit denoising problems. See Theory . SPGL1 is an open-source Matlab solver for sparse least-squares. It is designed to solve any one of these three problem formulations: Lasso problem \\mathop{\\mathrm{minimize}}_{x}\\quad {\\textstyle\\frac{1}{2}}\\Vert Ax-b\\Vert_2^2\\quad\\mathrm{subject\\ to}\\quad \\Vert x\\Vert_p \\leq \\tau \\mathop{\\mathrm{minimize}}_{x}\\quad {\\textstyle\\frac{1}{2}}\\Vert Ax-b\\Vert_2^2\\quad\\mathrm{subject\\ to}\\quad \\Vert x\\Vert_p \\leq \\tau Basis pursuit denoise \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_p\\quad \\mathrm{subject\\ to}\\quad \\Vert Ax-b\\Vert_2 \\leq \\sigma \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_p\\quad \\mathrm{subject\\ to}\\quad \\Vert Ax-b\\Vert_2 \\leq \\sigma Basis pursuit \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_p\\quad \\mathrm{subject\\ to}\\quad Ax=b \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_p\\quad \\mathrm{subject\\ to}\\quad Ax=b","title":"SPGL1: A solver for sparse least squares"},{"location":"#features","text":"Linear operator A. SPGL1 relies on matrix-vector operations A*x and A'*y , and accepts both explicit matrices (dense or sparse) and functions that evaluate these products. Sparsity regularizer. Any sparsity-inducing norm \\Vert\\cdot\\Vert_p \\Vert\\cdot\\Vert_p can be used if the user provides functions evaluating the primal norm, the corresponding dual norm, and a procedure for projecting onto a level-set of the primal norm. The default is the 1-norm. Several other norms included in SPGL1 are the group (1,2)-norm and the special multiple-measurement vector (MMV) case. Real and complex domains. SPGL1 is suitable for problems that live in either the real or complex domains. In the complex domain, the correct corresponding 1-norm (sum of magnitudes) is used.","title":"Features"},{"location":"#feedback","text":"We are glad to hear from you if you find SPGL1 useful, or if you have any suggestions, contributions, or bug reports. Please send these to the SPGL1 authors Ewout van den Berg (Email <vandenberg.ewout@gmail.com> ) Michael P. Friedlander (Email: <mpf@cs.ubc.ca> )","title":"Feedback"},{"location":"#credits","text":"This research is supported in part by an NSERC Discovery Grant and by a grant for the Office of Naval Research (N00014-17-1-2009).","title":"Credits"},{"location":"changelog/","text":"Change log v2.0 release, 18 Dec 2019 Implements quasi-Newton subspace search and dual root-finding mode for improved accuracy. v1.9 release, 29 Apr 2015 Recompute function information at new x when tau decreases. Fixed bug in subspace minimization. Thanks to Yang Lei for reporting this bug. v1.8 release, 25 May 2013 Fixed linesearch bug. v1.7 release, 20 May 2009 Fixed bug in MMV interface with implicit matrices. Thanks to Muhammad Usman for reporting this bug. v1.6 release, 20 August 2008 Added new fields to the options structure to specify functions for the evaluation of the primal and dual norm, and for Euclidean projection onto the associated primal norm ball. Added simplified interface +spg_mmv.m+ for the multiple measurement vector (MMV) version of BPDN, and +spg_group.m+ for group-sparse BPDN. v1.5 release, 17 June 2008 Added simplified interfaces for basis pursuit, basis pursuit denoise, and LASSO problems. Added html documentation. v1.4 release, 19 March 2008 Fixed bug in orthogonal projection onto the weighted one-norm ball. Thanks to Xiangrui Meng for reporting this bug. Added demo for basis pursuit with weighted one-norm to +spgdemo.m+ Added check to +spgl1.m+ to ensure \\tau \\tau is not updated in two consecutive iterations. Thanks to Gilles Hennenfent for reporting this bug. Added complexity check to projection code to ensure proper projection. v1.3 release, 29 January 2008 Changed stopping criterion for Lasso mode. Added code that allows restriction of the maximum number of matrix-vector operators (see +.maxMatvec+ option). Added safeguard +max(0,...)+ to ensure positivity when updating \\tau \\tau . This is needed when \\sigma \\sigma and the initial \\tau \\tau are both large.","title":"ChangeLog"},{"location":"changelog/#change-log","text":"v2.0 release, 18 Dec 2019 Implements quasi-Newton subspace search and dual root-finding mode for improved accuracy. v1.9 release, 29 Apr 2015 Recompute function information at new x when tau decreases. Fixed bug in subspace minimization. Thanks to Yang Lei for reporting this bug. v1.8 release, 25 May 2013 Fixed linesearch bug. v1.7 release, 20 May 2009 Fixed bug in MMV interface with implicit matrices. Thanks to Muhammad Usman for reporting this bug. v1.6 release, 20 August 2008 Added new fields to the options structure to specify functions for the evaluation of the primal and dual norm, and for Euclidean projection onto the associated primal norm ball. Added simplified interface +spg_mmv.m+ for the multiple measurement vector (MMV) version of BPDN, and +spg_group.m+ for group-sparse BPDN. v1.5 release, 17 June 2008 Added simplified interfaces for basis pursuit, basis pursuit denoise, and LASSO problems. Added html documentation. v1.4 release, 19 March 2008 Fixed bug in orthogonal projection onto the weighted one-norm ball. Thanks to Xiangrui Meng for reporting this bug. Added demo for basis pursuit with weighted one-norm to +spgdemo.m+ Added check to +spgl1.m+ to ensure \\tau \\tau is not updated in two consecutive iterations. Thanks to Gilles Hennenfent for reporting this bug. Added complexity check to projection code to ensure proper projection. v1.3 release, 29 January 2008 Changed stopping criterion for Lasso mode. Added code that allows restriction of the maximum number of matrix-vector operators (see +.maxMatvec+ option). Added safeguard +max(0,...)+ to ensure positivity when updating \\tau \\tau . This is needed when \\sigma \\sigma and the initial \\tau \\tau are both large.","title":"Change log"},{"location":"cite/","text":"Citing SPGL1 The description and theory of the SPGL1 algorithm is outlined in the paper E. van den Berg and M. P. Friedlander, Probing the Pareto frontier for basis pursuit solutions , SIAM J. on Scientific Computing, 31(2):890-912, November 2008. When referencing SPGL1, please cite the following two references: @misc { spgl1site , author = {E. van den Berg and M. P. Friedlander} , title = {{SPGL1}: A solver for large-scale sparse reconstruction} , note = {https://friedlander.io/spgl1} , month = {December} , year = 2019 } @article { BergFriedlander:2008 , Author = {E. van den Berg and M. P. Friedlander} , Title = {Probing the Pareto frontier for basis pursuit solutions} , year = {2008} , journal = {SIAM Journal on Scientific Computing} , volume = {31} , number = {2} , pages = {890-912} , url = {http://link.aip.org/link/?SCE/31/890} , doi = {10.1137/080714488} }","title":"Citing"},{"location":"cite/#citing-spgl1","text":"The description and theory of the SPGL1 algorithm is outlined in the paper E. van den Berg and M. P. Friedlander, Probing the Pareto frontier for basis pursuit solutions , SIAM J. on Scientific Computing, 31(2):890-912, November 2008. When referencing SPGL1, please cite the following two references: @misc { spgl1site , author = {E. van den Berg and M. P. Friedlander} , title = {{SPGL1}: A solver for large-scale sparse reconstruction} , note = {https://friedlander.io/spgl1} , month = {December} , year = 2019 } @article { BergFriedlander:2008 , Author = {E. van den Berg and M. P. Friedlander} , Title = {Probing the Pareto frontier for basis pursuit solutions} , year = {2008} , journal = {SIAM Journal on Scientific Computing} , volume = {31} , number = {2} , pages = {890-912} , url = {http://link.aip.org/link/?SCE/31/890} , doi = {10.1137/080714488} }","title":"Citing SPGL1"},{"location":"exits/","text":"Exit status The information structure returned by SPGL1 contains the exit code info.stat , along with a boolean flag that indicates whether the solve was successful or nor ( info.success ) and the error string info.statusStr . ====== ====================== ========= ========================================== Value Status code Success Status string ====== ====================== ========= ========================================== 1 ``EXIT_ROOT_FOUND`` Yes Found a root 2 ``EXIT_BPSOL_FOUND`` Yes Found a BP solution 3 ``EXIT_LEAST_SQUARES`` Yes Found a least-squares solution 4 ``EXIT_OPTIMAL`` Yes Optimal solution found 5 ``EXIT_ITERATIONS`` No Too many iterations 6 ``EXIT_LINE_ERROR`` No Linesearch error 7 ``EXIT_SUBOPTIMAL_BP`` No Found a suboptimal BP solution 8 ``EXIT_MATVEC_LIMIT`` No Maximum matrix-vector operations reached 9 ``EXIT_RUNTIME`` No Maximum runtime reached 10 ``EXIT_PROJECTION`` No Inaccurate projection ====== ====================== ========= ==========================================","title":"Exit Conditions"},{"location":"exits/#exit-status","text":"The information structure returned by SPGL1 contains the exit code info.stat , along with a boolean flag that indicates whether the solve was successful or nor ( info.success ) and the error string info.statusStr . ====== ====================== ========= ========================================== Value Status code Success Status string ====== ====================== ========= ========================================== 1 ``EXIT_ROOT_FOUND`` Yes Found a root 2 ``EXIT_BPSOL_FOUND`` Yes Found a BP solution 3 ``EXIT_LEAST_SQUARES`` Yes Found a least-squares solution 4 ``EXIT_OPTIMAL`` Yes Optimal solution found 5 ``EXIT_ITERATIONS`` No Too many iterations 6 ``EXIT_LINE_ERROR`` No Linesearch error 7 ``EXIT_SUBOPTIMAL_BP`` No Found a suboptimal BP solution 8 ``EXIT_MATVEC_LIMIT`` No Maximum matrix-vector operations reached 9 ``EXIT_RUNTIME`` No Maximum runtime reached 10 ``EXIT_PROJECTION`` No Inaccurate projection ====== ====================== ========= ==========================================","title":"Exit status"},{"location":"infostruct/","text":"Information structure =========== ================================================ Field Description =========== ================================================ tau Value of :math:`\\tau` in final Lasso subproblem rNorm Norm of the final residual gNorm Norm of the final gradient vector rGap Relative duality gap stat Exit status (see below) success Exit status (see below) statusStr Exit status (see below) iter Number of iterations nProdA Number of products with A nProdAt Number of products with A transpose nNewton Number of Newton root-finding iterations timeProject Time spent in the projection function timeMatProd Time spent in matrix-vector products timeTotal Total runtime options Options provided to the solver xNorm1 History of the primal norm of x (optional) rNorm2 History of the residual norm (optional) lambda History of the gradient norm (optional) =========== ================================================ The history fields are updated every iteration if options.history is set to true.","title":"Information Structure"},{"location":"infostruct/#information-structure","text":"=========== ================================================ Field Description =========== ================================================ tau Value of :math:`\\tau` in final Lasso subproblem rNorm Norm of the final residual gNorm Norm of the final gradient vector rGap Relative duality gap stat Exit status (see below) success Exit status (see below) statusStr Exit status (see below) iter Number of iterations nProdA Number of products with A nProdAt Number of products with A transpose nNewton Number of Newton root-finding iterations timeProject Time spent in the projection function timeMatProd Time spent in matrix-vector products timeTotal Total runtime options Options provided to the solver xNorm1 History of the primal norm of x (optional) rNorm2 History of the residual norm (optional) lambda History of the gradient norm (optional) =========== ================================================ The history fields are updated every iteration if options.history is set to true.","title":"Information structure"},{"location":"install/","text":"Download and Installation Download the distribution spgl1-2.1 (zip) Installation Unzip the distribution. This will create a directory called spgl1/ . We'll refer to this directory as <spgroot> . Start Matlab and execute the following commands from the Matlab prompt. >> addpath < spgroot > # Add Matlab to your path >> cd < spgroot > # Change directory >> spgsetup # Run SPGL1 ' s setup routine The spgsetup command compiles a fast C implementation of the projection routines. Compiling Matlab MEX interfaces is sometimes tricky business, and if your machine isn't setup for this, the spgsetup routine may fail. In that case, SPGL1 will default to using the precompiled interfaces that have been included. More information on how to change the default compiler is available here . To verify that the SPGL1 installation is working, execute the following command from within Matlab: >> spgdemo Source code The source code is maintained at https://github.com/mpf/spgl1 .","title":"Install & Download"},{"location":"install/#download-and-installation","text":"Download the distribution spgl1-2.1 (zip)","title":"Download and Installation"},{"location":"install/#installation","text":"Unzip the distribution. This will create a directory called spgl1/ . We'll refer to this directory as <spgroot> . Start Matlab and execute the following commands from the Matlab prompt. >> addpath < spgroot > # Add Matlab to your path >> cd < spgroot > # Change directory >> spgsetup # Run SPGL1 ' s setup routine The spgsetup command compiles a fast C implementation of the projection routines. Compiling Matlab MEX interfaces is sometimes tricky business, and if your machine isn't setup for this, the spgsetup routine may fail. In that case, SPGL1 will default to using the precompiled interfaces that have been included. More information on how to change the default compiler is available here . To verify that the SPGL1 installation is working, execute the following command from within Matlab: >> spgdemo","title":"Installation"},{"location":"install/#source-code","text":"The source code is maintained at https://github.com/mpf/spgl1 .","title":"Source code"},{"location":"options/","text":"Solver options Below we discuss the available options in the form options.<name> . These values can be set directly in the options structure, or can be provided as key-value pairs with <name>> as the key. In this example, the maximum number of iterations is set to 1000 and all output is suppressed via the verbosity level of 0: % Direct specification of parameters [ x , r , g , info ] = spgl1 ( A , b , tau , 'iterations' , 1000 , 'verbosity' , 0 ); Options can also be passed in via a single structure, which can be populated with default options via spgSetParams , or can be set directly: % Separate preparation of the options options = spgSetParams ( 'iterations' , 1000 , 'verbosity' , 0 ); [ x , r , g , info ] = spgl1 ( A , b , tau , options ); % Manual creation of the options structure options = struct (); options . iterations = 1000 ; options . verbosity = 0 ; [ x , r , g , info ] = spgl1 ( A , b , tau , options ); Individual parameters override the options structure: % Function call with mixed structure and key-value pair parameters [ x , r , g , info ] = spgl1 ( A , b , tau , options , 'mu' , 0 ); Superset options SPGL1 supports two predefined sets of parameters, called \"classic\" (default) and \"hybrid\". Among other options, the classic mode sets options.rootfindMode=1 (primal root finding) and options.hybridMode=false (no subspace search). The hybrid mode sets options.rootfindMode=1 (dual root finding) and options.hybridMode=true . Roughly, the \"classic\" mode is faster and less accurate than the \"hybrid\" mode. Please see the Theory section for further detail. These two examples show how to specify the superset options: % Use \"classic\" mode and set mu=0 [ x , r , g , info ] = spgl1 ( A , b , tau , 'classic' , 'mu' , 0 ); % Use \"hybrid\" mode [ x , r , g , info ] = spg_bpdn ( A , b , 'hybrid' ); The specific options for each mode are available here . Output options options.fid Gives the file identifier used for output. By default it is set to 1 (standard output), but it can be set to a file opened with fopen to redirect the output to a file. options.verbosity Controls the level of output ranging for no output (0) to all output (3). options.history Boolean flag that indicates whether the norms of the iterates, residuals, and gradient values should be recorded per iteration. By default the recording of the history is disabled. Resource limits options.iterations Gives the maximum allowed number of iteration. By default this value is set to ten times the number of rows in the operator A A . If the maximum number of iterations is reached, the solver returns with an EXIT_ITERATIONS error status. options.maxMatvec Gives the maximum allowed number of products with the operator A A or its adjoint. When exceeded, the solver returns with an EXIT_MATVEC_LIMIT error status. options.maxRuntime Gives the maximum runtime in seconds. To keep the overhead of checking to a minimum, the runtime limit is checked approximately every 0.5 seconds. When the maximum runtime is exceeded, the solver returns with an EXIT_RUNTIME error status. Problem settings options.iscomplex Indicates whether the problem has complex or real variable x x . SPGL1 can typically determine this automatically, but it helps to set the flag when dealing with complex variables. options.mu Specifies the regularization parameter \\mu \\mu . By default this value is set to 0. See regularization . Optimality conditions options.bpTol Specifies the tolerance for declaring a basis-pursuit solution (status EXIT_BPSOL_FOUND ) when solving a Lasso problem. The exact stopping criterion used is that the residual norm is less than options.bpTol times the norm of input parameter b. options.lsTol Specifies the tolerance for declaring a least-squares solution (status EXIT_LEAST_SQUARES ). The exact stopping criterion used is that the gradient norm is less than or equal to options.lsTol times the norm of the residual. options.optTol Specifies the tolerance for declaring an optimal solution (status EXIT_OPTIMAL ). This condition is met when the relative duality gap is less than or equal to options.optTol , where the relative duality gap is defined as the difference between the primal and dual solution divided by the maximum of the current primal value and options.relgapMinF . In basis-pursuit mode, this parameter is also used to determine when to initiate a root-finding step. An illustration of primal and dual feasible points for a Lasso subproblem is given below. Pareto curve along with primal and dual feasible points. options.projTol Specifies the tolerance for projection. This variable is mostly for internal use, and it set to options.optTol by default for backward compatibility. If the projection was found to be inaccurate the solver will exit with the EXIT_PROJECTION status. options.relgapMinF Specifies the relative duality gap. Tis value is obtained by dividing the primal-dual gap by the maximum of options.relgapMinF and the primal objective. By default, this parameter is set to 1. options.relgapMinR Specifies the relative residual error. This value is obtained by dividing the different between the residual norm and sigma by the maximum of options.relgapMinR and the residual norm. By default, this parameter is set to 1. Primal and dual norms The following options must be set together and provide a consistent combination of primal norm, dual norm, and orthogonal projection. The weights can be used as part of the norm, or set to the empty vector [] . options.weights Vector of weights for the primal norm. options.primal_norm Function that computes the primal norm of a given input vector, along with the weights. options.dual_norm Function that computes the dual norm of a given input vector, along with the weights for the primal norm. options.project Function that computes the orthogonal projection onto a ball of radius \\tau \\tau , as induced by the primal norm. Root-finding modes options.rootfindMode As explained in the Theory section , there are two modes for root finding: the primal root-finding mode and the dual root-finding mode. Set this option to 0 for the primal mode, and to 1 for the dual mode. options.rootfindTol In the dual root-finding mode, this parameter can be set to a value between 0 and 1 to indicate when root finding should be enabled. Values close to 1 require stricter sub-problem solves, whereas values closer to 0 allow relaxed subproblem solves, at the cost of a potentially larger number of sub-problems. By default this value is set to 0.5. Subspace minimization SPGL1 supports subspace minimization when solving (weighted) one-norm minimization problems. options.hybridMode Boolean parameter that specified whether subspace minimization should be activated. options.lbfgsHist Number of vectors in the L-BFGS approximation, when using subspace minimization. Line search options.nPrevVals Number of previous objective values used for non-monotone line search. options.stepMin Minimum step length for Barzilai-Borwein scaling. options.stepMax Maximum step length for Barzilai-Borwein scaling.","title":"Options"},{"location":"options/#solver-options","text":"Below we discuss the available options in the form options.<name> . These values can be set directly in the options structure, or can be provided as key-value pairs with <name>> as the key. In this example, the maximum number of iterations is set to 1000 and all output is suppressed via the verbosity level of 0: % Direct specification of parameters [ x , r , g , info ] = spgl1 ( A , b , tau , 'iterations' , 1000 , 'verbosity' , 0 ); Options can also be passed in via a single structure, which can be populated with default options via spgSetParams , or can be set directly: % Separate preparation of the options options = spgSetParams ( 'iterations' , 1000 , 'verbosity' , 0 ); [ x , r , g , info ] = spgl1 ( A , b , tau , options ); % Manual creation of the options structure options = struct (); options . iterations = 1000 ; options . verbosity = 0 ; [ x , r , g , info ] = spgl1 ( A , b , tau , options ); Individual parameters override the options structure: % Function call with mixed structure and key-value pair parameters [ x , r , g , info ] = spgl1 ( A , b , tau , options , 'mu' , 0 );","title":"Solver options"},{"location":"options/#superset-options","text":"SPGL1 supports two predefined sets of parameters, called \"classic\" (default) and \"hybrid\". Among other options, the classic mode sets options.rootfindMode=1 (primal root finding) and options.hybridMode=false (no subspace search). The hybrid mode sets options.rootfindMode=1 (dual root finding) and options.hybridMode=true . Roughly, the \"classic\" mode is faster and less accurate than the \"hybrid\" mode. Please see the Theory section for further detail. These two examples show how to specify the superset options: % Use \"classic\" mode and set mu=0 [ x , r , g , info ] = spgl1 ( A , b , tau , 'classic' , 'mu' , 0 ); % Use \"hybrid\" mode [ x , r , g , info ] = spg_bpdn ( A , b , 'hybrid' ); The specific options for each mode are available here .","title":"Superset options"},{"location":"options/#output-options","text":"options.fid Gives the file identifier used for output. By default it is set to 1 (standard output), but it can be set to a file opened with fopen to redirect the output to a file. options.verbosity Controls the level of output ranging for no output (0) to all output (3). options.history Boolean flag that indicates whether the norms of the iterates, residuals, and gradient values should be recorded per iteration. By default the recording of the history is disabled.","title":"Output options"},{"location":"options/#resource-limits","text":"options.iterations Gives the maximum allowed number of iteration. By default this value is set to ten times the number of rows in the operator A A . If the maximum number of iterations is reached, the solver returns with an EXIT_ITERATIONS error status. options.maxMatvec Gives the maximum allowed number of products with the operator A A or its adjoint. When exceeded, the solver returns with an EXIT_MATVEC_LIMIT error status. options.maxRuntime Gives the maximum runtime in seconds. To keep the overhead of checking to a minimum, the runtime limit is checked approximately every 0.5 seconds. When the maximum runtime is exceeded, the solver returns with an EXIT_RUNTIME error status.","title":"Resource limits"},{"location":"options/#problem-settings","text":"options.iscomplex Indicates whether the problem has complex or real variable x x . SPGL1 can typically determine this automatically, but it helps to set the flag when dealing with complex variables. options.mu Specifies the regularization parameter \\mu \\mu . By default this value is set to 0. See regularization .","title":"Problem settings"},{"location":"options/#optimality-conditions","text":"options.bpTol Specifies the tolerance for declaring a basis-pursuit solution (status EXIT_BPSOL_FOUND ) when solving a Lasso problem. The exact stopping criterion used is that the residual norm is less than options.bpTol times the norm of input parameter b. options.lsTol Specifies the tolerance for declaring a least-squares solution (status EXIT_LEAST_SQUARES ). The exact stopping criterion used is that the gradient norm is less than or equal to options.lsTol times the norm of the residual. options.optTol Specifies the tolerance for declaring an optimal solution (status EXIT_OPTIMAL ). This condition is met when the relative duality gap is less than or equal to options.optTol , where the relative duality gap is defined as the difference between the primal and dual solution divided by the maximum of the current primal value and options.relgapMinF . In basis-pursuit mode, this parameter is also used to determine when to initiate a root-finding step. An illustration of primal and dual feasible points for a Lasso subproblem is given below. Pareto curve along with primal and dual feasible points. options.projTol Specifies the tolerance for projection. This variable is mostly for internal use, and it set to options.optTol by default for backward compatibility. If the projection was found to be inaccurate the solver will exit with the EXIT_PROJECTION status. options.relgapMinF Specifies the relative duality gap. Tis value is obtained by dividing the primal-dual gap by the maximum of options.relgapMinF and the primal objective. By default, this parameter is set to 1. options.relgapMinR Specifies the relative residual error. This value is obtained by dividing the different between the residual norm and sigma by the maximum of options.relgapMinR and the residual norm. By default, this parameter is set to 1.","title":"Optimality conditions"},{"location":"options/#primal-and-dual-norms","text":"The following options must be set together and provide a consistent combination of primal norm, dual norm, and orthogonal projection. The weights can be used as part of the norm, or set to the empty vector [] . options.weights Vector of weights for the primal norm. options.primal_norm Function that computes the primal norm of a given input vector, along with the weights. options.dual_norm Function that computes the dual norm of a given input vector, along with the weights for the primal norm. options.project Function that computes the orthogonal projection onto a ball of radius \\tau \\tau , as induced by the primal norm.","title":"Primal and dual norms"},{"location":"options/#root-finding-modes","text":"options.rootfindMode As explained in the Theory section , there are two modes for root finding: the primal root-finding mode and the dual root-finding mode. Set this option to 0 for the primal mode, and to 1 for the dual mode. options.rootfindTol In the dual root-finding mode, this parameter can be set to a value between 0 and 1 to indicate when root finding should be enabled. Values close to 1 require stricter sub-problem solves, whereas values closer to 0 allow relaxed subproblem solves, at the cost of a potentially larger number of sub-problems. By default this value is set to 0.5.","title":"Root-finding modes"},{"location":"options/#subspace-minimization","text":"SPGL1 supports subspace minimization when solving (weighted) one-norm minimization problems. options.hybridMode Boolean parameter that specified whether subspace minimization should be activated. options.lbfgsHist Number of vectors in the L-BFGS approximation, when using subspace minimization.","title":"Subspace minimization"},{"location":"options/#line-search","text":"options.nPrevVals Number of previous objective values used for non-monotone line search. options.stepMin Minimum step length for Barzilai-Borwein scaling. options.stepMax Maximum step length for Barzilai-Borwein scaling.","title":"Line search"},{"location":"pareto/","text":"Pareto curve The Pareto curve plays a central role in SPGL1. It is given as the lowest residual norm, as a function of \\tau \\tau , which bounds the maximum allowable primal norm of the iterate x x , i.e., \\|x\\|_p\\le\\tau \\|x\\|_p\\le\\tau . Example Pareto curve (blue) along with a desired misfit level \\(\\sigma\\) and Lasso parameter \\(\\tau\\) that attains the desired solution. Solving the Lasso problem for a given value of \\tau \\tau amounts to evaluating a point on the curve. The basis-pursuit denoise problem is more difficult: we are given \\sigma \\sigma and need to find the value of \\tau \\tau where the values of the Pareto curve attains the desired value. Root finding SPGL1 solves the basis-pursuit problem by solving a sequence of Lasso problems with different values of \\tau \\tau . Given the solution of a Lasso problem, it determines the gradient of the Pareto curve at that point, which allows us to do root-finding. As an illustration, suppose we start at \\tau=0 \\tau=0 and solve the Lasso problem for this value (this always gives x=0 x=0 ). We then evaluate the gradient, which leaves us at the situation shown in the left plot below. We then update \\tau \\tau to the point where thelinear approximation to the Pareto curve intersect with the desired value of \\sigma \\sigma , indicated by the red dot. In this case this gives, say \\tau= 1.2 \\tau= 1.2 . We then solve the Lasso problem with the new value of \\tau \\tau , as shown in the right plot, and repeate the procedure until the solution has a residual norm that is sufficiently close to \\sigma \\sigma . Implementation SPGL1 provides options that control the root-finding process. The most important option, options.rootfindMode , specified whether root finding is done using the primal or the dual mode, with parameter values 0 and 1 respectively. Primal mode When using the primal objective, SPGL1 uses relaxed conditions to determine whether the Lasso subproblem is solved and approximates the gradient based on the primal objective. This root-finding mode can be very fast, but may result in value of \\tau \\tau that exceeds the minimum. As a result, this mode is useful when a quick solution is needed, or for large-scale problems where highly accurate solves of the subproblem are prohibitive. An exaggerated illustration of root-finding on the Pareto curve in the primal mode is given in the image below. The subproblem is solved approximately for \\tau=1.2 \\tau=1.2 . The solution (indicated by the gray point) may be slighly above the Pareto curve, or the gradient may be too flat. This makes it possible to 'overshoot' the target and obtain a solution with a value \\tau \\tau that exceeds the minimum possible value. Dual mode In the dual root-finding mode we apply root finding based on linear under-approximation of the Pareto curve. Provided that the initial \\tau \\tau is below the final value (a value of zero is always highly recommended), there is never any overestimation. As illustrated below, we use the dual objective value (indicated by the gray dot) along with a gradient to determine the next value of \\tau \\tau . A new value for the intersection with the desired misfit level \\sigma \\sigma is computed at every iteration and the maximum value is maintained for the next root-finding step. Root finding is initiated when the duality gap is a fraction ( options.rootfindTol ) of the difference between \\sigma \\sigma and the primal objective. As the overall gap shrinks towards the solution, increasingly accurate solves are automatically enforced.","title":"Theory"},{"location":"pareto/#pareto-curve","text":"The Pareto curve plays a central role in SPGL1. It is given as the lowest residual norm, as a function of \\tau \\tau , which bounds the maximum allowable primal norm of the iterate x x , i.e., \\|x\\|_p\\le\\tau \\|x\\|_p\\le\\tau . Example Pareto curve (blue) along with a desired misfit level \\(\\sigma\\) and Lasso parameter \\(\\tau\\) that attains the desired solution. Solving the Lasso problem for a given value of \\tau \\tau amounts to evaluating a point on the curve. The basis-pursuit denoise problem is more difficult: we are given \\sigma \\sigma and need to find the value of \\tau \\tau where the values of the Pareto curve attains the desired value.","title":"Pareto curve"},{"location":"pareto/#root-finding","text":"SPGL1 solves the basis-pursuit problem by solving a sequence of Lasso problems with different values of \\tau \\tau . Given the solution of a Lasso problem, it determines the gradient of the Pareto curve at that point, which allows us to do root-finding. As an illustration, suppose we start at \\tau=0 \\tau=0 and solve the Lasso problem for this value (this always gives x=0 x=0 ). We then evaluate the gradient, which leaves us at the situation shown in the left plot below. We then update \\tau \\tau to the point where thelinear approximation to the Pareto curve intersect with the desired value of \\sigma \\sigma , indicated by the red dot. In this case this gives, say \\tau= 1.2 \\tau= 1.2 . We then solve the Lasso problem with the new value of \\tau \\tau , as shown in the right plot, and repeate the procedure until the solution has a residual norm that is sufficiently close to \\sigma \\sigma .","title":"Root finding"},{"location":"pareto/#implementation","text":"SPGL1 provides options that control the root-finding process. The most important option, options.rootfindMode , specified whether root finding is done using the primal or the dual mode, with parameter values 0 and 1 respectively.","title":"Implementation"},{"location":"pareto/#primal-mode","text":"When using the primal objective, SPGL1 uses relaxed conditions to determine whether the Lasso subproblem is solved and approximates the gradient based on the primal objective. This root-finding mode can be very fast, but may result in value of \\tau \\tau that exceeds the minimum. As a result, this mode is useful when a quick solution is needed, or for large-scale problems where highly accurate solves of the subproblem are prohibitive. An exaggerated illustration of root-finding on the Pareto curve in the primal mode is given in the image below. The subproblem is solved approximately for \\tau=1.2 \\tau=1.2 . The solution (indicated by the gray point) may be slighly above the Pareto curve, or the gradient may be too flat. This makes it possible to 'overshoot' the target and obtain a solution with a value \\tau \\tau that exceeds the minimum possible value.","title":"Primal mode"},{"location":"pareto/#dual-mode","text":"In the dual root-finding mode we apply root finding based on linear under-approximation of the Pareto curve. Provided that the initial \\tau \\tau is below the final value (a value of zero is always highly recommended), there is never any overestimation. As illustrated below, we use the dual objective value (indicated by the gray dot) along with a gradient to determine the next value of \\tau \\tau . A new value for the intersection with the desired misfit level \\sigma \\sigma is computed at every iteration and the maximum value is maintained for the next root-finding step. Root finding is initiated when the duality gap is a fraction ( options.rootfindTol ) of the difference between \\sigma \\sigma and the primal objective. As the overall gap shrinks towards the solution, increasingly accurate solves are automatically enforced.","title":"Dual mode"},{"location":"regularization/","text":"Regularization Regularize version of Lasso and basis-pursuit denoise can be obtained by augmenting A A with the weighted identity matrix, and augmenting b b with a vector of all zero. For convenience, SPGL1 also supports direct regularization, which changes the Lasso formulation to \\mathop{\\mathrm{minimize}}_{x}\\quad {\\textstyle\\frac{1}{2}}\\Vert Ax-b\\Vert_2^2 + {\\textstyle\\frac{\\mu}{2}}\\Vert x\\Vert_2^2\\quad\\mathrm{subject\\ to}\\quad \\Vert x\\Vert_p \\leq \\tau \\mathop{\\mathrm{minimize}}_{x}\\quad {\\textstyle\\frac{1}{2}}\\Vert Ax-b\\Vert_2^2 + {\\textstyle\\frac{\\mu}{2}}\\Vert x\\Vert_2^2\\quad\\mathrm{subject\\ to}\\quad \\Vert x\\Vert_p \\leq \\tau and basis-pursuit denoise to \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_p\\quad \\mathrm{subject\\ to}\\quad \\left\\Vert \\left[\\begin{array}{c}A\\\\ \\sqrt{\\mu}I\\end{array}\\right]x-\\left[\\begin{array}{c}b\\\\0\\end{array}\\right]\\right\\Vert_2\\leq \\sigma. \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_p\\quad \\mathrm{subject\\ to}\\quad \\left\\Vert \\left[\\begin{array}{c}A\\\\ \\sqrt{\\mu}I\\end{array}\\right]x-\\left[\\begin{array}{c}b\\\\0\\end{array}\\right]\\right\\Vert_2\\leq \\sigma. The \\mu \\mu parameter can be specified in the options as options.mu or as parameter mu .","title":"Regularization"},{"location":"regularization/#regularization","text":"Regularize version of Lasso and basis-pursuit denoise can be obtained by augmenting A A with the weighted identity matrix, and augmenting b b with a vector of all zero. For convenience, SPGL1 also supports direct regularization, which changes the Lasso formulation to \\mathop{\\mathrm{minimize}}_{x}\\quad {\\textstyle\\frac{1}{2}}\\Vert Ax-b\\Vert_2^2 + {\\textstyle\\frac{\\mu}{2}}\\Vert x\\Vert_2^2\\quad\\mathrm{subject\\ to}\\quad \\Vert x\\Vert_p \\leq \\tau \\mathop{\\mathrm{minimize}}_{x}\\quad {\\textstyle\\frac{1}{2}}\\Vert Ax-b\\Vert_2^2 + {\\textstyle\\frac{\\mu}{2}}\\Vert x\\Vert_2^2\\quad\\mathrm{subject\\ to}\\quad \\Vert x\\Vert_p \\leq \\tau and basis-pursuit denoise to \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_p\\quad \\mathrm{subject\\ to}\\quad \\left\\Vert \\left[\\begin{array}{c}A\\\\ \\sqrt{\\mu}I\\end{array}\\right]x-\\left[\\begin{array}{c}b\\\\0\\end{array}\\right]\\right\\Vert_2\\leq \\sigma. \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_p\\quad \\mathrm{subject\\ to}\\quad \\left\\Vert \\left[\\begin{array}{c}A\\\\ \\sqrt{\\mu}I\\end{array}\\right]x-\\left[\\begin{array}{c}b\\\\0\\end{array}\\right]\\right\\Vert_2\\leq \\sigma. The \\mu \\mu parameter can be specified in the options as options.mu or as parameter mu .","title":"Regularization"},{"location":"usage/","text":"User's guide Lasso The standard Lasso problem, given by \\mathop{\\mathrm{minimize}}_{x}\\quad {\\textstyle\\frac{1}{2}}\\Vert Ax-b\\Vert_2^2\\quad\\mathrm{subject\\ to}\\quad \\Vert x\\Vert_1 \\leq \\tau, \\mathop{\\mathrm{minimize}}_{x}\\quad {\\textstyle\\frac{1}{2}}\\Vert Ax-b\\Vert_2^2\\quad\\mathrm{subject\\ to}\\quad \\Vert x\\Vert_1 \\leq \\tau, can be solved by SPGL1 using the spg_lasso function: [ x , r , g , info ] = spg_lasso ( A , b , tau , options ) The options parameter controls the stopping criteria and behavior of the solver and consists of keyword-value pairs or structures (see solver options for more details). Default options are used when no options are specified. As an example, let's create random matrix A A and vector b b , and solve the Lasso problem for \\tau=0.5 \\tau=0.5 using: s = RandStream ( 'mt19937ar' , 'Seed' , 0 ); A = randn ( s , 5 , 10 ); b = randn ( s , 5 , 1 ); [ x , r , g , info ] = spg_lasso ( A , b , 0.5 ); The SPGL1 solver gives an output similar to this: =================================================================== SPGL1 v2.0 (28 Nov 2019) =================================================================== No. rows : 5 Initial tau : 5.00e-01 No. columns : 10 2-norm of b : 1.86e+00 Optimality tol : 1.00e-04 bound on 1-norm of x: 5.00e-01 Basis pursuit tol : 1.00e-04 Maximum iterations : 50 Iter ||Ax-b||_2 Relative Gap ||A'r||_d 0 1.8616905e+00 9.7024931e-01 3.36e+00 1 1.4495790e+00 9.5092862e-01 2.74e+00 2 1.3023687e+00 6.1655207e-01 2.30e+00 3 1.2009939e+00 1.7815336e-01 1.43e+00 4 1.1775752e+00 1.5030186e-01 1.55e+00 5 1.1558273e+00 1.2492861e-01 1.34e+00 6 1.1513362e+00 7.1502516e-02 1.28e+00 7 1.1492592e+00 4.1721960e-02 1.22e+00 8 1.1465284e+00 3.8571207e-02 1.28e+00 9 1.1491912e+00 4.1627659e-02 1.47e+00 10 1.1486180e+00 4.0969188e-02 1.44e+00 11 1.1446930e+00 4.3987941e-03 1.25e+00 12 1.1446779e+00 4.0502125e-03 1.25e+00 13 1.1446237e+00 1.5756440e-04 1.25e+00 14 1.1446236e+00 2.5627973e-05 1.25e+00 EXIT -- Optimal solution found Products with A : 14 Total time (secs) : 0.1 Products with A' : 15 Project time (secs) : 0.0 Newton iterations : 0 Mat-vec time (secs) : 0.0 Line search its : 0 The exit status indicates that an optimal solution was found. The output of the solver is given by the tuple [x,r,g,info] with the following fields: the computed solution x the residual r = b-Ax the gradient g := -A'r of the objective at the solution an info structure that contains information such as the exit status, the number of iterations, and the total run time; see Information Structure for details. Continuing with our example, we find that if we solve Lasso with \\tau=1 \\tau=1 , the required number of iterations exceed the default of 10 times the number of rows in A A . We can increase the maximum number of iterations to 100 by calling [ x , r , g , info ] = spg_lasso ( A , b , 1 , 'iterations' , 100 ); In this case around 90 iterations suffice to find an optimal solution. Basis-pursuit denoise The standard basis-pursuit denoise problem is given by \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_1\\quad \\mathrm{subject\\ to}\\quad \\Vert Ax-b\\Vert_2 \\leq \\sigma. \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_1\\quad \\mathrm{subject\\ to}\\quad \\Vert Ax-b\\Vert_2 \\leq \\sigma. Solving the problem in SPGL1 can be done by calling the spg_bpdn function [ x , r , g , info ] = spg_bpdn ( A , b , sigma , options ) In the special case where \\sigma=0 \\sigma=0 , the problem reduces to the basis pursuit problem \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_1\\quad \\mathrm{subject\\ to}\\quad Ax=b, \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_1\\quad \\mathrm{subject\\ to}\\quad Ax=b, which can be solved using the basis-pursuit solver: [ x , r , g , info ] = spg_bp ( A , b , options ) The basis-pursuit solution is obtained by solving a sequence of Lasso problems with suitable values of \\tau \\tau . The gradient output g thus refers to the objective value of the Lasso subproblem corresponding to the latest value of \\tau \\tau . As an example, consider the same example used for the Lasso problem, now solved with basis-pursuit denoise for \\sigma=1.0 \\sigma=1.0 : s = RandStream ( 'mt19937ar' , 'Seed' , 0 ); A = randn ( s , 5 , 10 ); b = randn ( s , 5 , 1 ); [ x , r , g , info ] = spg_bpdn ( A , b , 1.0 ); The solver gives an output similar to =================================================================== SPGL1 v2.0 (28 Nov 2019) =================================================================== No. rows : 5 Initial tau : 0.00e+00 No. columns : 10 2-norm of b : 1.86e+00 Optimality tol : 1.00e-04 Target ||Ax-b||_2 : 1.00e+00 Basis pursuit tol : 1.00e-04 Maximum iterations : 50 Iter ||Ax-b||_2 Relative Gap Rel Error ||A'r||_d ||x||_p 0 1.8616905e+00 0.0000000e+00 4.63e-01 3.36e+00 4.77e-01 1 1.4973610e+00 5.2616060e-01 3.32e-01 2.12e+00 2 1.2974269e+00 1.9995584e-01 2.29e-01 1.62e+00 3 1.2030579e+00 8.1971765e-02 1.69e-01 1.77e+00 4 1.1995095e+00 7.7709077e-02 1.66e-01 1.91e+00 6.25e-01 5 1.0272341e+00 8.8274756e-02 2.65e-02 1.11e+00 6 1.0212485e+00 8.2144096e-02 2.08e-02 1.09e+00 7 1.0187911e+00 2.8385712e-02 1.84e-02 9.66e-01 8 1.0183832e+00 2.2393424e-02 1.81e-02 9.64e-01 6.45e-01 9 9.9870605e-01 1.6574628e-02 1.29e-03 9.45e-01 10 1.0008367e+00 1.8704785e-02 8.36e-04 1.11e+00 11 1.0070820e+00 2.4974860e-02 7.03e-03 1.30e+00 12 9.9794627e-01 1.5816119e-02 2.05e-03 1.01e+00 13 9.9785653e-01 9.7287655e-03 2.14e-03 9.87e-01 6.42e-01 14 9.9993939e-01 9.6116065e-03 6.06e-05 9.92e-01 EXIT -- Found a root Products with A : 18 Total time (secs) : 0.1 Products with A' : 16 Project time (secs) : 0.0 Newton iterations : 4 Mat-vec time (secs) : 0.0 Line search its : 3 The last column of the output shows that four Lasso subproblems with different values of \\tau \\tau are solved to obtain the basis-pursuit denoise solution. Group norms SPGL1 supports group-sparse versions of the three main problem classes (Lasso, basis-pursuit denoise, and basis pursuit). The group norm is defined by given subsets of the vector elements, such that each element occurs in exactly one subset. The norm is then define as the sum of the Euclidean norms of the subvectors in each set. To solve the group-norm basis-pursuit denoise formulation we can use [x,r,g,info] = spg_group(A,b,groups,sigma,options) The groups parameter is a vector that contains the group number for each of the elements in x . The group numbers themselves can be chosen arbitrary, as long as elements in the same group have the same number. Groups do not have to consist of contiguous elements, although in practice they often are. In the following example we use three groups, labeled 1 , 2 , and 3 : s = RandStream ( 'mt19937ar' , 'Seed' , 0 ); A = randn ( s , 5 , 10 ); b = randn ( s , 5 , 1 ); sigma = 1.2 x = spg_group ( A , b ,[ 1 , 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 , 3 ], sigma ); In this case we get a result that is group-sparse: the elements in some groups are all zero, whereas those in other groups are all non-zero: x = 0 0 0 -0.1804 -0.2038 -0.1107 0.0037 0 0 0 Multiple measurement vectors (MMV) A special case of group sparsity is the multiple-measurement vectors (MMV) problem. In this problem we are given a matrix of measurements B , and assume that the unknown matrix X is such that all columns have the same support. This is often achieved by minimizing the sum of Euclidean norms of the rows in X . This can be reformulated as a group-norm problem by appropriately vectorizing B and X , and suitably redefining A . For convenience SPGL1 provides the function [X,R,G,info] = spg_mmv(A,B,sigma,options); As an example, we solve s = RandStream ( 'mt19937ar' , 'Seed' , 0 ); A = randn ( s , 5 , 7 ); B = randn ( s , 5 , 6 ); [ X , R , G , info ] = spg_mmv ( A , B , 3.5 ); which gives X = 0.1102 -0.0296 -0.0177 0.0365 -0.0298 -0.0218 0.0123 0.0042 0.1348 -0.0577 -0.0208 0.1842 -0.0020 -0.0001 -0.0008 0.0009 -0.0087 0.0027 0 0 0 0 0 0 0.0806 0.0252 0.0703 -0.1165 0.0985 0.0176 0 0 0 0 0 0 0.2815 0.1036 -0.1306 0.0197 -0.0596 -0.2179 Generic interface The generic interface to SPGL1 is given by [ x , r , g , info ] = spgl1 ( A , b , tau , sigma , x0 , options ) [ x , r , g , info ] = spgl1 ( A , b , tau , sigma , options ) [ x , r , g , info ] = spgl1 ( A , b , tau , options ) The options parameters are optional and can be a mixture of structure objects and key-value pairs. The first option parameter can be a string identifying a predefined parameter set, which can then be modified by the parameters that follow. See the examples in options . The x0 parameter can be provided to initialize x x . If set, the parameters tau and sigma must also be provided. To solve the Lasso problem formulation with an initial value for x , set sigma to the empty vector [] . In order to solve basis pursuit denoise we can set tau to 0 or an empty vector [] (strongly recommended), or provide an initial value for tau (generally not recommended). In case an initial value of tau is specified, it is important that this value be smaller than the value for tau at the solution; reduction of tau from a value that is too large can be very time consuming or result in a suboptimal basis-pursuit solution. When the x0 parameter is set to the empty vector [] , a default initial value for x x is used. Custom norms SPGL1 can be extended to solve Lasso and basis-pursuit denoise problems with custom primal norms. For this we need to provide three functions options.primal_norm , which evaluates the primal norm \\|x\\|_p \\|x\\|_p of a given vector x x ; options.dual_norm , which evaluates the dual norm \\|y\\|_d \\|y\\|_d corresponding to the primal norm; options.project , which evaluates orthogonal projection onto a scaled unit-norm ball corresponding to the primal norm, i.e., onto the set \\mathcal{B}_p:=\\{x \\mid \\|x\\|_p\\le\\tau\\} \\mathcal{B}_p:=\\{x \\mid \\|x\\|_p\\le\\tau\\} for some positive value \\tau \\tau . Good examples of the definition of these norms can be found in the spg_group solver.","title":"Usage"},{"location":"usage/#users-guide","text":"","title":"User's guide"},{"location":"usage/#lasso","text":"The standard Lasso problem, given by \\mathop{\\mathrm{minimize}}_{x}\\quad {\\textstyle\\frac{1}{2}}\\Vert Ax-b\\Vert_2^2\\quad\\mathrm{subject\\ to}\\quad \\Vert x\\Vert_1 \\leq \\tau, \\mathop{\\mathrm{minimize}}_{x}\\quad {\\textstyle\\frac{1}{2}}\\Vert Ax-b\\Vert_2^2\\quad\\mathrm{subject\\ to}\\quad \\Vert x\\Vert_1 \\leq \\tau, can be solved by SPGL1 using the spg_lasso function: [ x , r , g , info ] = spg_lasso ( A , b , tau , options ) The options parameter controls the stopping criteria and behavior of the solver and consists of keyword-value pairs or structures (see solver options for more details). Default options are used when no options are specified. As an example, let's create random matrix A A and vector b b , and solve the Lasso problem for \\tau=0.5 \\tau=0.5 using: s = RandStream ( 'mt19937ar' , 'Seed' , 0 ); A = randn ( s , 5 , 10 ); b = randn ( s , 5 , 1 ); [ x , r , g , info ] = spg_lasso ( A , b , 0.5 ); The SPGL1 solver gives an output similar to this: =================================================================== SPGL1 v2.0 (28 Nov 2019) =================================================================== No. rows : 5 Initial tau : 5.00e-01 No. columns : 10 2-norm of b : 1.86e+00 Optimality tol : 1.00e-04 bound on 1-norm of x: 5.00e-01 Basis pursuit tol : 1.00e-04 Maximum iterations : 50 Iter ||Ax-b||_2 Relative Gap ||A'r||_d 0 1.8616905e+00 9.7024931e-01 3.36e+00 1 1.4495790e+00 9.5092862e-01 2.74e+00 2 1.3023687e+00 6.1655207e-01 2.30e+00 3 1.2009939e+00 1.7815336e-01 1.43e+00 4 1.1775752e+00 1.5030186e-01 1.55e+00 5 1.1558273e+00 1.2492861e-01 1.34e+00 6 1.1513362e+00 7.1502516e-02 1.28e+00 7 1.1492592e+00 4.1721960e-02 1.22e+00 8 1.1465284e+00 3.8571207e-02 1.28e+00 9 1.1491912e+00 4.1627659e-02 1.47e+00 10 1.1486180e+00 4.0969188e-02 1.44e+00 11 1.1446930e+00 4.3987941e-03 1.25e+00 12 1.1446779e+00 4.0502125e-03 1.25e+00 13 1.1446237e+00 1.5756440e-04 1.25e+00 14 1.1446236e+00 2.5627973e-05 1.25e+00 EXIT -- Optimal solution found Products with A : 14 Total time (secs) : 0.1 Products with A' : 15 Project time (secs) : 0.0 Newton iterations : 0 Mat-vec time (secs) : 0.0 Line search its : 0 The exit status indicates that an optimal solution was found. The output of the solver is given by the tuple [x,r,g,info] with the following fields: the computed solution x the residual r = b-Ax the gradient g := -A'r of the objective at the solution an info structure that contains information such as the exit status, the number of iterations, and the total run time; see Information Structure for details. Continuing with our example, we find that if we solve Lasso with \\tau=1 \\tau=1 , the required number of iterations exceed the default of 10 times the number of rows in A A . We can increase the maximum number of iterations to 100 by calling [ x , r , g , info ] = spg_lasso ( A , b , 1 , 'iterations' , 100 ); In this case around 90 iterations suffice to find an optimal solution.","title":"Lasso"},{"location":"usage/#basis-pursuit-denoise","text":"The standard basis-pursuit denoise problem is given by \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_1\\quad \\mathrm{subject\\ to}\\quad \\Vert Ax-b\\Vert_2 \\leq \\sigma. \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_1\\quad \\mathrm{subject\\ to}\\quad \\Vert Ax-b\\Vert_2 \\leq \\sigma. Solving the problem in SPGL1 can be done by calling the spg_bpdn function [ x , r , g , info ] = spg_bpdn ( A , b , sigma , options ) In the special case where \\sigma=0 \\sigma=0 , the problem reduces to the basis pursuit problem \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_1\\quad \\mathrm{subject\\ to}\\quad Ax=b, \\mathop{\\mathrm{minimize}}_{x}\\quad \\Vert x\\Vert_1\\quad \\mathrm{subject\\ to}\\quad Ax=b, which can be solved using the basis-pursuit solver: [ x , r , g , info ] = spg_bp ( A , b , options ) The basis-pursuit solution is obtained by solving a sequence of Lasso problems with suitable values of \\tau \\tau . The gradient output g thus refers to the objective value of the Lasso subproblem corresponding to the latest value of \\tau \\tau . As an example, consider the same example used for the Lasso problem, now solved with basis-pursuit denoise for \\sigma=1.0 \\sigma=1.0 : s = RandStream ( 'mt19937ar' , 'Seed' , 0 ); A = randn ( s , 5 , 10 ); b = randn ( s , 5 , 1 ); [ x , r , g , info ] = spg_bpdn ( A , b , 1.0 ); The solver gives an output similar to =================================================================== SPGL1 v2.0 (28 Nov 2019) =================================================================== No. rows : 5 Initial tau : 0.00e+00 No. columns : 10 2-norm of b : 1.86e+00 Optimality tol : 1.00e-04 Target ||Ax-b||_2 : 1.00e+00 Basis pursuit tol : 1.00e-04 Maximum iterations : 50 Iter ||Ax-b||_2 Relative Gap Rel Error ||A'r||_d ||x||_p 0 1.8616905e+00 0.0000000e+00 4.63e-01 3.36e+00 4.77e-01 1 1.4973610e+00 5.2616060e-01 3.32e-01 2.12e+00 2 1.2974269e+00 1.9995584e-01 2.29e-01 1.62e+00 3 1.2030579e+00 8.1971765e-02 1.69e-01 1.77e+00 4 1.1995095e+00 7.7709077e-02 1.66e-01 1.91e+00 6.25e-01 5 1.0272341e+00 8.8274756e-02 2.65e-02 1.11e+00 6 1.0212485e+00 8.2144096e-02 2.08e-02 1.09e+00 7 1.0187911e+00 2.8385712e-02 1.84e-02 9.66e-01 8 1.0183832e+00 2.2393424e-02 1.81e-02 9.64e-01 6.45e-01 9 9.9870605e-01 1.6574628e-02 1.29e-03 9.45e-01 10 1.0008367e+00 1.8704785e-02 8.36e-04 1.11e+00 11 1.0070820e+00 2.4974860e-02 7.03e-03 1.30e+00 12 9.9794627e-01 1.5816119e-02 2.05e-03 1.01e+00 13 9.9785653e-01 9.7287655e-03 2.14e-03 9.87e-01 6.42e-01 14 9.9993939e-01 9.6116065e-03 6.06e-05 9.92e-01 EXIT -- Found a root Products with A : 18 Total time (secs) : 0.1 Products with A' : 16 Project time (secs) : 0.0 Newton iterations : 4 Mat-vec time (secs) : 0.0 Line search its : 3 The last column of the output shows that four Lasso subproblems with different values of \\tau \\tau are solved to obtain the basis-pursuit denoise solution.","title":"Basis-pursuit denoise"},{"location":"usage/#group-norms","text":"SPGL1 supports group-sparse versions of the three main problem classes (Lasso, basis-pursuit denoise, and basis pursuit). The group norm is defined by given subsets of the vector elements, such that each element occurs in exactly one subset. The norm is then define as the sum of the Euclidean norms of the subvectors in each set. To solve the group-norm basis-pursuit denoise formulation we can use [x,r,g,info] = spg_group(A,b,groups,sigma,options) The groups parameter is a vector that contains the group number for each of the elements in x . The group numbers themselves can be chosen arbitrary, as long as elements in the same group have the same number. Groups do not have to consist of contiguous elements, although in practice they often are. In the following example we use three groups, labeled 1 , 2 , and 3 : s = RandStream ( 'mt19937ar' , 'Seed' , 0 ); A = randn ( s , 5 , 10 ); b = randn ( s , 5 , 1 ); sigma = 1.2 x = spg_group ( A , b ,[ 1 , 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 , 3 ], sigma ); In this case we get a result that is group-sparse: the elements in some groups are all zero, whereas those in other groups are all non-zero: x = 0 0 0 -0.1804 -0.2038 -0.1107 0.0037 0 0 0","title":"Group norms"},{"location":"usage/#multiple-measurement-vectors-mmv","text":"A special case of group sparsity is the multiple-measurement vectors (MMV) problem. In this problem we are given a matrix of measurements B , and assume that the unknown matrix X is such that all columns have the same support. This is often achieved by minimizing the sum of Euclidean norms of the rows in X . This can be reformulated as a group-norm problem by appropriately vectorizing B and X , and suitably redefining A . For convenience SPGL1 provides the function [X,R,G,info] = spg_mmv(A,B,sigma,options); As an example, we solve s = RandStream ( 'mt19937ar' , 'Seed' , 0 ); A = randn ( s , 5 , 7 ); B = randn ( s , 5 , 6 ); [ X , R , G , info ] = spg_mmv ( A , B , 3.5 ); which gives X = 0.1102 -0.0296 -0.0177 0.0365 -0.0298 -0.0218 0.0123 0.0042 0.1348 -0.0577 -0.0208 0.1842 -0.0020 -0.0001 -0.0008 0.0009 -0.0087 0.0027 0 0 0 0 0 0 0.0806 0.0252 0.0703 -0.1165 0.0985 0.0176 0 0 0 0 0 0 0.2815 0.1036 -0.1306 0.0197 -0.0596 -0.2179","title":"Multiple measurement vectors (MMV)"},{"location":"usage/#generic-interface","text":"The generic interface to SPGL1 is given by [ x , r , g , info ] = spgl1 ( A , b , tau , sigma , x0 , options ) [ x , r , g , info ] = spgl1 ( A , b , tau , sigma , options ) [ x , r , g , info ] = spgl1 ( A , b , tau , options ) The options parameters are optional and can be a mixture of structure objects and key-value pairs. The first option parameter can be a string identifying a predefined parameter set, which can then be modified by the parameters that follow. See the examples in options . The x0 parameter can be provided to initialize x x . If set, the parameters tau and sigma must also be provided. To solve the Lasso problem formulation with an initial value for x , set sigma to the empty vector [] . In order to solve basis pursuit denoise we can set tau to 0 or an empty vector [] (strongly recommended), or provide an initial value for tau (generally not recommended). In case an initial value of tau is specified, it is important that this value be smaller than the value for tau at the solution; reduction of tau from a value that is too large can be very time consuming or result in a suboptimal basis-pursuit solution. When the x0 parameter is set to the empty vector [] , a default initial value for x x is used.","title":"Generic interface"},{"location":"usage/#custom-norms","text":"SPGL1 can be extended to solve Lasso and basis-pursuit denoise problems with custom primal norms. For this we need to provide three functions options.primal_norm , which evaluates the primal norm \\|x\\|_p \\|x\\|_p of a given vector x x ; options.dual_norm , which evaluates the dual norm \\|y\\|_d \\|y\\|_d corresponding to the primal norm; options.project , which evaluates orthogonal projection onto a scaled unit-norm ball corresponding to the primal norm, i.e., onto the set \\mathcal{B}_p:=\\{x \\mid \\|x\\|_p\\le\\tau\\} \\mathcal{B}_p:=\\{x \\mid \\|x\\|_p\\le\\tau\\} for some positive value \\tau \\tau . Good examples of the definition of these norms can be found in the spg_group solver.","title":"Custom norms"}]}